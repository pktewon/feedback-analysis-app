import os
import io
import re
from typing import Optional, List, Tuple, Dict

import pandas as pd
import streamlit as st

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text as sklearn_text

# VADER sentiment analyzer (lightweight)
_vader = None


APP_TITLE = "ê³ ê° í”¼ë“œë°± ë¶„ì„"
DEFAULT_FILE_NAME = "@feedback-data.csv"


def find_default_csv_file() -> Optional[str]:
    workspace_dir = os.getcwd()
    candidate_path = os.path.join(workspace_dir, DEFAULT_FILE_NAME)
    if os.path.exists(candidate_path):
        return candidate_path
    return None


@st.cache_data(show_spinner=False, ttl=300)  # 5ë¶„ ìºì‹œ
def load_dataframe(file_like_or_path) -> pd.DataFrame:
    if isinstance(file_like_or_path, (str, os.PathLike)):
        return pd.read_csv(file_like_or_path)
    else:
        file_like_or_path.seek(0)
        return pd.read_csv(file_like_or_path)


def ensure_text_column_ui(df: pd.DataFrame) -> Tuple[str, Optional[str], Optional[str]]:
    st.subheader("ë°ì´í„° ì»¬ëŸ¼ ë§¤í•‘")
    cols = list(df.columns)
    # ì¶”ì •: í…ìŠ¤íŠ¸ë¡œ ë³´ì´ëŠ” ì»¬ëŸ¼ ìš°ì„  ì„ íƒ
    default_text_col = None
    for c in cols:
        lc = str(c).lower()
        if any(k in lc for k in ["text", "ë‚´ìš©", "í”¼ë“œë°±", "comment", "ë¦¬ë·°", "review"]):
            default_text_col = c
            break
    text_col = st.selectbox("í…ìŠ¤íŠ¸ ì»¬ëŸ¼", options=cols, index=(cols.index(default_text_col) if default_text_col in cols else 0))

    # ë‚ ì§œ/ì¹´í…Œê³ ë¦¬(ì œí’ˆêµ°) ì»¬ëŸ¼ ì„ íƒì€ ì„ íƒ ì‚¬í•­
    date_col = st.selectbox("ë‚ ì§œ ì»¬ëŸ¼(ì„ íƒ)", options=["(ì—†ìŒ)"] + cols)
    category_col = st.selectbox("ì¹´í…Œê³ ë¦¬/ì œí’ˆêµ° ì»¬ëŸ¼(ì„ íƒ)", options=["(ì—†ìŒ)"] + cols)

    date_col = None if date_col == "(ì—†ìŒ)" else date_col
    category_col = None if category_col == "(ì—†ìŒ)" else category_col
    return text_col, date_col, category_col


def apply_filters(df: pd.DataFrame, date_col: Optional[str], category_col: Optional[str]) -> pd.DataFrame:
    filtered = df.copy()
    if date_col and date_col in filtered.columns:
        with st.expander("ê¸°ê°„ í•„í„°"):
            # ë‚ ì§œ íŒŒì‹± ì‹œë„
            parsed = pd.to_datetime(filtered[date_col], errors="coerce")
            filtered = filtered.assign(_parsed_date=parsed)
            min_d = filtered["_parsed_date"].min()
            max_d = filtered["_parsed_date"].max()
            if pd.notna(min_d) and pd.notna(max_d):
                start, end = st.date_input("ë‚ ì§œ ë²”ìœ„", value=(min_d.date(), max_d.date()))
                mask = (filtered["_parsed_date"].dt.date >= start) & (filtered["_parsed_date"].dt.date <= end)
                filtered = filtered.loc[mask]
            filtered = filtered.drop(columns=["_parsed_date"], errors="ignore")

    if category_col and category_col in filtered.columns:
        with st.expander("ì¹´í…Œê³ ë¦¬ í•„í„°"):
            cats = [c for c in sorted(filtered[category_col].dropna().astype(str).unique())]
            selected = st.multiselect("ì¹´í…Œê³ ë¦¬ ì„ íƒ", options=cats, default=cats)
            filtered = filtered[filtered[category_col].astype(str).isin(selected)]

    return filtered


@st.cache_resource
def load_vader():
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    return SentimentIntensityAnalyzer()


# í•œêµ­ì–´ ê°ì„± ë¶„ì„ì„ ìœ„í•œ ê°ì • ì‚¬ì „
KOREAN_POSITIVE_WORDS = {
    'ì¢‹', 'ì¢‹ì€', 'ì¢‹ë‹¤', 'ì¢‹ì•„', 'ì¢‹ìŠµë‹ˆë‹¤', 'ì¢‹ì•„ìš”', 'í›Œë¥­', 'í›Œë¥­í•œ', 'í›Œë¥­í•˜ë‹¤', 'í›Œë¥­í•´', 'í›Œë¥­í•©ë‹ˆë‹¤',
    'ì™„ë²½', 'ì™„ë²½í•œ', 'ì™„ë²½í•˜ë‹¤', 'ì™„ë²½í•´', 'ì™„ë²½í•©ë‹ˆë‹¤', 'ì™„ë²½í•´ìš”', 'ìµœê³ ', 'ìµœê³ ë‹¤', 'ìµœê³ ì…ë‹ˆë‹¤', 'ìµœê³ ì˜ˆìš”',
    'ë§Œì¡±', 'ë§Œì¡±ìŠ¤ëŸ½', 'ë§Œì¡±ìŠ¤ëŸ¬ìš´', 'ë§Œì¡±ìŠ¤ëŸ½ë‹¤', 'ë§Œì¡±ìŠ¤ëŸ¬ì›Œ', 'ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤', 'ë§Œì¡±í•´', 'ë§Œì¡±í•©ë‹ˆë‹¤',
    'ì¶”ì²œ', 'ì¶”ì²œí•´', 'ì¶”ì²œí•©ë‹ˆë‹¤', 'ì¶”ì²œë“œë ¤', 'ì¶”ì²œë“œë¦½ë‹ˆë‹¤', 'ì¶”ì²œí•´ìš”', 'ì¶”ì²œí•´ë“œë ¤ìš”',
    'ê°ì‚¬', 'ê°ì‚¬í•´', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê°ì‚¬í•´ìš”', 'ê³ ë§ˆì›Œ', 'ê³ ë§ˆì›Œìš”', 'ê³ ë§™ìŠµë‹ˆë‹¤',
    'ì‚¬ë‘', 'ì‚¬ë‘í•´', 'ì‚¬ë‘í•©ë‹ˆë‹¤', 'ì‚¬ë‘í•´ìš”', 'ì¢‹ì•„í•´', 'ì¢‹ì•„í•©ë‹ˆë‹¤', 'ì¢‹ì•„í•´ìš”',
    'í–‰ë³µ', 'í–‰ë³µí•´', 'í–‰ë³µí•©ë‹ˆë‹¤', 'í–‰ë³µí•´ìš”', 'ê¸°ì˜', 'ê¸°ì˜ë‹¤', 'ê¸°ë»', 'ê¸°ë»ìš”', 'ê¸°ì©ë‹ˆë‹¤',
    'ì‹ ë‚˜', 'ì‹ ë‚˜ë‹¤', 'ì‹ ë‚˜ìš”', 'ì‹ ë‚©ë‹ˆë‹¤', 'ì¬ë¯¸ìˆ', 'ì¬ë¯¸ìˆë‹¤', 'ì¬ë¯¸ìˆì–´', 'ì¬ë¯¸ìˆì–´ìš”', 'ì¬ë¯¸ìˆìŠµë‹ˆë‹¤',
    'í¸ë¦¬', 'í¸ë¦¬í•œ', 'í¸ë¦¬í•˜ë‹¤', 'í¸ë¦¬í•´', 'í¸ë¦¬í•©ë‹ˆë‹¤', 'í¸ë¦¬í•´ìš”', 'ì‰¬', 'ì‰½ë‹¤', 'ì‰¬ì›Œ', 'ì‰¬ì›Œìš”', 'ì‰½ìŠµë‹ˆë‹¤',
    'ë¹ ë¥´', 'ë¹ ë¥´ë‹¤', 'ë¹¨ë¼', 'ë¹¨ë¼ìš”', 'ë¹ ë¦…ë‹ˆë‹¤', 'ë¹ ë¥´ê²Œ', 'ë¹¨ë¦¬', 'íš¨ìœ¨', 'íš¨ìœ¨ì ', 'íš¨ìœ¨ì ì´ë‹¤', 'íš¨ìœ¨ì ì´ì—ìš”',
    'ìš°ìˆ˜', 'ìš°ìˆ˜í•œ', 'ìš°ìˆ˜í•˜ë‹¤', 'ìš°ìˆ˜í•´', 'ìš°ìˆ˜í•©ë‹ˆë‹¤', 'ìš°ìˆ˜í•´ìš”', 'í›Œë¥­', 'í›Œë¥­í•œ', 'í›Œë¥­í•˜ë‹¤', 'í›Œë¥­í•´',
    'ë›°ì–´ë‚˜', 'ë›°ì–´ë‚œ', 'ë›°ì–´ë‚˜ë‹¤', 'ë›°ì–´ë‚˜ìš”', 'ë›°ì–´ë‚©ë‹ˆë‹¤', 'íƒì›”', 'íƒì›”í•œ', 'íƒì›”í•˜ë‹¤', 'íƒì›”í•´',
    'í›Œë¥­', 'í›Œë¥­í•œ', 'í›Œë¥­í•˜ë‹¤', 'í›Œë¥­í•´', 'í›Œë¥­í•©ë‹ˆë‹¤', 'í›Œë¥­í•´ìš”', 'í›Œë¥­í•´ìš”', 'í›Œë¥­í•´ìš”',
    'ì •ë§', 'ì •ë§ë¡œ', 'ì§„ì§œ', 'ì§„ì‹¬', 'ì™„ì „', 'ì™„ì „íˆ', 'ë„ˆë¬´', 'ë§¤ìš°', 'ì •ë§', 'ì •ë§ë¡œ', 'ì§„ì§œ', 'ì§„ì‹¬',
    'excellent', 'great', 'good', 'amazing', 'wonderful', 'fantastic', 'awesome', 'perfect', 'best', 'love'
}

KOREAN_NEGATIVE_WORDS = {
    'ë‚˜ì˜', 'ë‚˜ì˜ë‹¤', 'ë‚˜ë¹ ', 'ë‚˜ë¹ ìš”', 'ë‚˜ì©ë‹ˆë‹¤', 'ë‚˜ìœ', 'ì•ˆì¢‹', 'ì•ˆì¢‹ë‹¤', 'ì•ˆì¢‹ì•„', 'ì•ˆì¢‹ì•„ìš”', 'ì•ˆì¢‹ìŠµë‹ˆë‹¤',
    'ìµœì•…', 'ìµœì•…ì´ë‹¤', 'ìµœì•…ì´ì—ìš”', 'ìµœì•…ì…ë‹ˆë‹¤', 'ìµœì•…ì´ì•¼', 'ìµœì•…ì´ì•¼ìš”', 'ìµœì•…ì´ì—ìš”',
    'ë¶ˆë§Œ', 'ë¶ˆë§ŒìŠ¤ëŸ½', 'ë¶ˆë§ŒìŠ¤ëŸ¬ìš´', 'ë¶ˆë§ŒìŠ¤ëŸ½ë‹¤', 'ë¶ˆë§ŒìŠ¤ëŸ¬ì›Œ', 'ë¶ˆë§ŒìŠ¤ëŸ½ìŠµë‹ˆë‹¤', 'ë¶ˆë§Œì´', 'ë¶ˆë§Œì´ì—ìš”',
    'í™”ë‚˜', 'í™”ë‚˜ë‹¤', 'í™”ë‚˜ìš”', 'í™”ë‚©ë‹ˆë‹¤', 'í™”ê°€', 'í™”ê°€ë‚˜', 'í™”ê°€ë‚˜ìš”', 'í™”ê°€ë‚©ë‹ˆë‹¤', 'ì§œì¦', 'ì§œì¦ë‚˜', 'ì§œì¦ë‚˜ìš”',
    'ì‹¤ë§', 'ì‹¤ë§ìŠ¤ëŸ½', 'ì‹¤ë§ìŠ¤ëŸ¬ìš´', 'ì‹¤ë§ìŠ¤ëŸ½ë‹¤', 'ì‹¤ë§ìŠ¤ëŸ¬ì›Œ', 'ì‹¤ë§ìŠ¤ëŸ½ìŠµë‹ˆë‹¤', 'ì‹¤ë§í•´', 'ì‹¤ë§í•©ë‹ˆë‹¤',
    'ìŠ¬í”„', 'ìŠ¬í”„ë‹¤', 'ìŠ¬í¼', 'ìŠ¬í¼ìš”', 'ìŠ¬í”•ë‹ˆë‹¤', 'ìš°ìš¸', 'ìš°ìš¸í•˜ë‹¤', 'ìš°ìš¸í•´', 'ìš°ìš¸í•´ìš”', 'ìš°ìš¸í•©ë‹ˆë‹¤',
    'í˜ë“¤', 'í˜ë“¤ë‹¤', 'í˜ë“¤ì–´', 'í˜ë“¤ì–´ìš”', 'í˜ë“­ë‹ˆë‹¤', 'ì–´ë µ', 'ì–´ë µë‹¤', 'ì–´ë ¤ì›Œ', 'ì–´ë ¤ì›Œìš”', 'ì–´ë µìŠµë‹ˆë‹¤',
    'ë³µì¡', 'ë³µì¡í•œ', 'ë³µì¡í•˜ë‹¤', 'ë³µì¡í•´', 'ë³µì¡í•©ë‹ˆë‹¤', 'ë³µì¡í•´ìš”', 'ë¶ˆí¸', 'ë¶ˆí¸í•œ', 'ë¶ˆí¸í•˜ë‹¤', 'ë¶ˆí¸í•´',
    'ëŠë¦¬', 'ëŠë¦¬ë‹¤', 'ëŠë ¤', 'ëŠë ¤ìš”', 'ëŠë¦½ë‹ˆë‹¤', 'ëŠë¦¬ê²Œ', 'ì²œì²œíˆ', 'ë¹„íš¨ìœ¨', 'ë¹„íš¨ìœ¨ì ', 'ë¹„íš¨ìœ¨ì ì´ë‹¤',
    'ë¬¸ì œ', 'ë¬¸ì œê°€', 'ë¬¸ì œê°€ìˆ', 'ë¬¸ì œê°€ìˆë‹¤', 'ë¬¸ì œê°€ìˆì–´', 'ë¬¸ì œê°€ìˆì–´ìš”', 'ë¬¸ì œê°€ìˆìŠµë‹ˆë‹¤', 'ë¬¸ì œìˆ', 'ë¬¸ì œìˆë‹¤',
    'ì´ìƒ', 'ì´ìƒí•˜ë‹¤', 'ì´ìƒí•´', 'ì´ìƒí•´ìš”', 'ì´ìƒí•©ë‹ˆë‹¤', 'ì´ìƒí•œ', 'ì´ìƒí•˜ë„¤', 'ì´ìƒí•˜ë„¤ìš”',
    'ê±°ì§“', 'ê±°ì§“ë§', 'ê±°ì§“ë§ì´', 'ê±°ì§“ë§ì´ì—ìš”', 'ê±°ì§“ë§ì…ë‹ˆë‹¤', 'ê±°ì§“ì´', 'ê±°ì§“ì´ì—ìš”', 'ê±°ì§“ì…ë‹ˆë‹¤',
    'ê°€ì§œ', 'ê°€ì§œë‹¤', 'ê°€ì§œì•¼', 'ê°€ì§œì˜ˆìš”', 'ê°€ì§œì…ë‹ˆë‹¤', 'ê°€ì§œë„¤', 'ê°€ì§œë„¤ìš”',
    'ì‹«', 'ì‹«ë‹¤', 'ì‹«ì–´', 'ì‹«ì–´ìš”', 'ì‹«ìŠµë‹ˆë‹¤', 'ì‹«ì–´í•´', 'ì‹«ì–´í•´ìš”', 'ì‹«ì–´í•©ë‹ˆë‹¤',
    'í˜ì˜¤', 'í˜ì˜¤ìŠ¤ëŸ½', 'í˜ì˜¤ìŠ¤ëŸ¬ìš´', 'í˜ì˜¤ìŠ¤ëŸ½ë‹¤', 'í˜ì˜¤ìŠ¤ëŸ¬ì›Œ', 'í˜ì˜¤ìŠ¤ëŸ½ìŠµë‹ˆë‹¤', 'í˜ì˜¤í•´', 'í˜ì˜¤í•©ë‹ˆë‹¤',
    'bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'dislike', 'disappointed', 'angry', 'sad'
}

KOREAN_INTENSIFIERS = {
    'ì •ë§', 'ì§„ì§œ', 'ì™„ì „', 'ì™„ì „íˆ', 'ë„ˆë¬´', 'ë§¤ìš°', 'ì •ë§ë¡œ', 'ì§„ì‹¬', 'ì •ë§ë¡œ', 'ì§„ì§œë¡œ', 'ì™„ì „íˆ', 'ì™„ì „',
    'ì—„ì²­', 'ì—„ì²­ë‚˜', 'ì—„ì²­ë‚˜ê²Œ', 'ì—„ì²­ë‚œ', 'ì—„ì²­ë‚˜ë‹¤', 'ì—„ì²­ë‚˜ìš”', 'ì—„ì²­ë‚©ë‹ˆë‹¤',
    'ì•„ì£¼', 'ì•„ì£¼ë„', 'ì•„ì£¼ë„', 'ì•„ì£¼ë„', 'ì•„ì£¼ë„', 'ì•„ì£¼ë„', 'ì•„ì£¼ë„', 'ì•„ì£¼ë„',
    'so', 'very', 'really', 'extremely', 'absolutely', 'totally', 'completely'
}


def analyze_korean_sentiment(text: str) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    if not text or pd.isna(text):
        return "ì¤‘ë¦½"
    
    text = str(text).lower().strip()
    if len(text) < 2:
        return "ì¤‘ë¦½"
    
    # ê¸ì •/ë¶€ì • ë‹¨ì–´ ì¹´ìš´íŠ¸
    positive_score = 0
    negative_score = 0
    
    # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì í¬í•¨)
    words = re.findall(r'[\wê°€-í£]+', text)
    
    for word in words:
        # ê¸ì • ë‹¨ì–´ ì²´í¬
        if any(pos_word in word for pos_word in KOREAN_POSITIVE_WORDS):
            # ê°•ì¡°ì–´ê°€ ì•ì— ìˆëŠ”ì§€ í™•ì¸
            word_index = text.find(word)
            if word_index > 0:
                prev_text = text[:word_index].strip()
                if any(intensifier in prev_text for intensifier in KOREAN_INTENSIFIERS):
                    positive_score += 2  # ê°•ì¡°ì–´ê°€ ìˆìœ¼ë©´ 2ì 
                else:
                    positive_score += 1
            else:
                positive_score += 1
        
        # ë¶€ì • ë‹¨ì–´ ì²´í¬
        if any(neg_word in word for neg_word in KOREAN_NEGATIVE_WORDS):
            # ê°•ì¡°ì–´ê°€ ì•ì— ìˆëŠ”ì§€ í™•ì¸
            word_index = text.find(word)
            if word_index > 0:
                prev_text = text[:word_index].strip()
                if any(intensifier in prev_text for intensifier in KOREAN_INTENSIFIERS):
                    negative_score += 2  # ê°•ì¡°ì–´ê°€ ìˆìœ¼ë©´ 2ì 
                else:
                    negative_score += 1
            else:
                negative_score += 1
    
    # ë¶€ì • í‘œí˜„ ì²´í¬ (ì˜ˆ: "ì•ˆ ì¢‹ë‹¤", "ë³„ë¡œë‹¤", "ì•„ë‹ˆë‹¤")
    negative_patterns = [
        r'ì•ˆ\s*ì¢‹', r'ë³„ë¡œ', r'ì•„ë‹ˆ', r'ëª»', r'ì—†', r'ì•„ë‹Œ', r'ì•„ë‹™ë‹ˆë‹¤', r'ì•„ë‹ˆì—ìš”',
        r'not\s+good', r'not\s+great', r'not\s+excellent', r'not\s+amazing'
    ]
    
    for pattern in negative_patterns:
        if re.search(pattern, text):
            negative_score += 1
    
    # ê¸ì • í‘œí˜„ ì²´í¬ (ì˜ˆ: "ì •ë§ ì¢‹ë‹¤", "ì™„ì „ ì¢‹ë‹¤")
    positive_patterns = [
        r'ì •ë§\s*ì¢‹', r'ì™„ì „\s*ì¢‹', r'ì§„ì§œ\s*ì¢‹', r'ë„ˆë¬´\s*ì¢‹', r'ë§¤ìš°\s*ì¢‹',
        r'really\s+good', r'very\s+good', r'so\s+good', r'extremely\s+good'
    ]
    
    for pattern in positive_patterns:
        if re.search(pattern, text):
            positive_score += 1
    
    # ê°ì„± íŒì •
    if positive_score > negative_score and positive_score > 0:
        return "ê¸ì •"
    elif negative_score > positive_score and negative_score > 0:
        return "ë¶€ì •"
    else:
        return "ì¤‘ë¦½"


@st.cache_data(ttl=600)  # 10ë¶„ ìºì‹œ
def analyze_sentiment(texts: List[str], method: str) -> List[str]:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì˜ ê°ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    labels = []
    for text in texts:
        if not text or pd.isna(text):
            labels.append("ì¤‘ë¦½")
            continue
            
        # í•œêµ­ì–´ ê°ì„± ë¶„ì„ ì‚¬ìš©
        sentiment = analyze_korean_sentiment(text)
        labels.append(sentiment)
    
    return labels


@st.cache_data(ttl=600)  # 10ë¶„ ìºì‹œ
def extract_keywords(texts: List[str], top_k: int = 20) -> pd.DataFrame:
    # í•œêµ­ì–´+ì˜ë¬¸ í† í° íŒ¨í„´, í•œê¸€/ì˜ë¬¸/ìˆ«ì 2ì ì´ìƒ
    token_pattern = r"(?u)([\wê°€-í£]{2,})"
    stop_words = sklearn_text.ENGLISH_STOP_WORDS.union({
        "ì œí’ˆ", "ì‚¬ìš©", "ê³ ê°", "ì„œë¹„ìŠ¤", "ë¬¸ì œ", "ì´ìŠˆ", "ë¬¸ì˜", "ê°ì‚¬", "ì •ë§",
        "í•©ë‹ˆë‹¤", "ìˆì–´ìš”", "ìˆìŠµë‹ˆë‹¤", "ê²ƒ", "ë•Œ", "ì¢€", "ë„ˆë¬´", "ë§¤ìš°", "ì •ë„"
    })

    vectorizer = TfidfVectorizer(
        token_pattern=token_pattern,
        stop_words=stop_words,
        ngram_range=(1, 2),
        min_df=2
    )
    docs = [str(t) if pd.notna(t) else "" for t in texts]
    if len(docs) == 0:
        return pd.DataFrame(columns=["term", "score"])

    try:
        X = vectorizer.fit_transform(docs)
    except ValueError:
        # í† í°ì´ ì—†ì„ ë•Œ
        return pd.DataFrame(columns=["term", "score"])

    scores = X.sum(axis=0).A1
    terms = vectorizer.get_feature_names_out()
    df_scores = pd.DataFrame({"term": terms, "score": scores})
    df_scores = df_scores.sort_values("score", ascending=False).head(top_k)
    return df_scores.reset_index(drop=True)


def show_visualizations(df: pd.DataFrame, sentiment_col: str, text_col: str):
    st.subheader("ğŸ“ˆ ì‹œê°í™”")
    
    # ê°ì„± ë¶„í¬ ì°¨íŠ¸
    col1, col2 = st.columns([2, 1])
    with col1:
        counts = df[sentiment_col].value_counts().reindex(["ë¶€ì •", "ì¤‘ë¦½", "ê¸ì •"]).fillna(0).astype(int)
        st.bar_chart(counts)
    
    with col2:
        # ê°ì„± ë¶„í¬ íŒŒì´ ì°¨íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        sentiment_data = counts.to_dict()
        st.write("**ê°ì„± ë¶„í¬**")
        for sentiment, count in sentiment_data.items():
            percentage = (count / counts.sum()) * 100
            st.write(f"â€¢ {sentiment}: {count}ê°œ ({percentage:.1f}%)")

    # í‚¤ì›Œë“œ ë¶„ì„
    with st.expander("ğŸ” ìƒìœ„ í‚¤ì›Œë“œ ë¶„ì„", expanded=True):
        kw_df = extract_keywords(df[text_col].tolist(), top_k=30)
        if len(kw_df) == 0:
            st.info("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.dataframe(kw_df, use_container_width=True)
            
            # í‚¤ì›Œë“œ ì ìˆ˜ ì°¨íŠ¸
            if len(kw_df) > 0:
                st.bar_chart(kw_df.set_index('term')['score'])


def main():
    st.set_page_config(
        page_title=APP_TITLE, 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    st.title("ğŸ“Š " + APP_TITLE)
    st.caption("ğŸ’¡ CSV ì—…ë¡œë“œ ë˜ëŠ” í”„ë¡œì íŠ¸ í´ë”ì˜ '@feedback-data.csv' ìë™ ë¶ˆëŸ¬ì˜¤ê¸° ì§€ì›")
    
    # ì•± ì„¤ëª…
    with st.expander("â„¹ï¸ ì•± ì‚¬ìš©ë²•", expanded=False):
        st.markdown("""
        **ì´ ì•±ì˜ ê¸°ëŠ¥:**
        1. **ë°ì´í„° ì—…ë¡œë“œ**: CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©
        2. **ì»¬ëŸ¼ ë§¤í•‘**: í…ìŠ¤íŠ¸, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì„ íƒ
        3. **í•„í„°ë§**: ë‚ ì§œ ë²”ìœ„ ë° ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
        4. **ê°ì„± ë¶„ì„**: VADER ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ë¹ ë¥¸ ê°ì„± ë¶„ì„
        5. **í‚¤ì›Œë“œ ì¶”ì¶œ**: TF-IDF ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ ë¶„ì„
        6. **ì‹œê°í™”**: ê°ì„± ë¶„í¬ ì°¨íŠ¸ ë° í‚¤ì›Œë“œ í…Œì´ë¸”
        7. **ê²°ê³¼ ë‹¤ìš´ë¡œë“œ**: ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        """)

    # ë°ì´í„° ì…ë ¥ ì„¹ì…˜
    with st.sidebar:
        st.header("ğŸ“ ë°ì´í„° ì…ë ¥")
        uploaded = st.file_uploader("CSV ì—…ë¡œë“œ", type=["csv"], help="CSV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”")
        default_path = find_default_csv_file()
        if default_path:
            st.success(f"âœ… ê¸°ë³¸ íŒŒì¼ ê°ì§€: {DEFAULT_FILE_NAME}")
        else:
            st.info("ğŸ’¡ í”„ë¡œì íŠ¸ í´ë”ì— '@feedback-data.csv'ë¥¼ ë‘ë©´ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤")
        
        st.divider()
        st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
        method = st.selectbox("ê°ì„± ë¶„ì„ ë°©ë²•", ["í•œêµ­ì–´ ê°ì„± ë¶„ì„ (ê²½ëŸ‰)"], help="í•œêµ­ì–´ì— ìµœì í™”ëœ ê°ì„± ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤")
        run_btn = st.button("ğŸš€ ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True)

    df: Optional[pd.DataFrame] = None
    source_label = None
    if uploaded is not None:
        try:
            df = load_dataframe(uploaded)
            source_label = "ì—…ë¡œë“œ íŒŒì¼"
        except Exception as e:
            st.error(f"ì—…ë¡œë“œ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
    elif default_path:
        try:
            df = load_dataframe(default_path)
            source_label = DEFAULT_FILE_NAME
        except Exception as e:
            st.error(f"ê¸°ë³¸ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")

    if df is None:
        st.info("ğŸ“‹ ì¢Œì¸¡ì—ì„œ CSVë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, í”„ë¡œì íŠ¸ í´ë”ì— '@feedback-data.csv'ë¥¼ ë‘ì„¸ìš”.")
        return

    # ë°ì´í„° ë¡œë”© ì„±ê³µ ë©”ì‹œì§€
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        st.success(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ ({source_label})")
    with col2:
        st.metric("ì´ í–‰ ìˆ˜", f"{len(df):,}")
    with col3:
        st.metric("ì´ ì»¬ëŸ¼ ìˆ˜", len(df.columns))
    
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    with st.expander("ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=True):
        st.dataframe(df.head(20), use_container_width=True)

    text_col, date_col, category_col = ensure_text_column_ui(df)
    df_filtered = apply_filters(df, date_col, category_col)

    if run_btn:
        if text_col not in df_filtered.columns:
            st.error("ìœ íš¨í•œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.")
            return

        with st.spinner("ğŸ”„ ê°ì„± ë¶„ì„ ì¤‘..."):
            sentiments = analyze_sentiment(df_filtered[text_col].tolist(), method)
            result_df = df_filtered.copy()
            result_df = result_df.assign(_sentiment=sentiments)

        st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
        
        # ë¶„ì„ ê²°ê³¼ ìš”ì•½
        sentiment_counts = result_df['_sentiment'].value_counts()
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ê¸ì •", sentiment_counts.get('ê¸ì •', 0))
        with col2:
            st.metric("ì¤‘ë¦½", sentiment_counts.get('ì¤‘ë¦½', 0))
        with col3:
            st.metric("ë¶€ì •", sentiment_counts.get('ë¶€ì •', 0))
        with col4:
            st.metric("ì´ ë¶„ì„ ìˆ˜", len(result_df))

        st.subheader("ğŸ“‹ ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        display_cols = [text_col, *([date_col] if date_col else []), *([category_col] if category_col else []), "_sentiment"]
        st.dataframe(result_df[display_cols].head(50), use_container_width=True)

        show_visualizations(result_df, sentiment_col="_sentiment", text_col=text_col)

        # ë‹¤ìš´ë¡œë“œ
        st.subheader("ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
        csv_buf = io.StringIO()
        result_df.to_csv(csv_buf, index=False)
        st.download_button(
            label="ğŸ“¥ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_buf.getvalue(),
            file_name="feedback_analysis_results.csv",
            mime="text/csv",
            type="primary",
            use_container_width=True
        )
    else:
        st.info("ğŸš€ ì¢Œì¸¡ì˜ 'ë¶„ì„ ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì‹œì‘í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()


